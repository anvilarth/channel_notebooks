{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytorch_lightning lightning_utilities torchmetrics tqdm pyyaml matplotlib\n",
    "# %pip install nvidia-dali-cuda120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali.plugin.pytorch import (\n",
    "    DALIGenericIterator as PyTorchIterator,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import decode_image, decode_jpeg\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds_data = \"/home/a.filatov/workdir/dataset_loading/tar_repo/0.tar\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharded_pipeline(device_id, shard_id, num_shards):\n",
    "    pipe = Pipeline(batch_size=batch_size, num_threads=batch_size, device_id=device_id)\n",
    "    with pipe:\n",
    "        img_raw = fn.readers.webdataset(\n",
    "            paths=wds_data, ext=[\"jpeg;png;jpg\"], missing_component_behavior=\"skip\", dtypes=types.UINT8,             \n",
    "            shard_id=shard_id,\n",
    "            num_shards=num_shards\n",
    "        )\n",
    "        img = fn.decoders.image(img_raw, device=\"mixed\", output_type=types.RGB)\n",
    "        img = fn.resize(img, device=\"gpu\", resize_x=1024, resize_y=1024)\n",
    "        img = fn.crop_mirror_normalize(\n",
    "            img,\n",
    "            dtype=types.FLOAT16,\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[255.0, 255.0, 255.0],\n",
    "            scale=2,\n",
    "            shift=-1,\n",
    "        )\n",
    "\n",
    "        pipe.set_outputs(img,)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = sharded_pipeline(0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.filatov/workdir/miniconda3/envs/afilatov_fast_loading/lib/python3.12/site-packages/nvidia/dali/plugin/base_iterator.py:208: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may lead, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.\n",
      "  _iterator_deprecation_warning()\n",
      "[/opt/dali/dali/operators/reader/loader/webdataset_loader.cc:380] Index file not provided, it may take some time to infer it from the tar file\n"
     ]
    }
   ],
   "source": [
    "dali_iter = DALIGenericIterator([pipe], [\"images\"], size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional block: input channels 3, output channels 32\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        # Second convolutional block: input channels 32, output channels 64\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # Third convolutional block: input channels 64, output channels 128\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # After three pooling operations, the image size reduces from 1024 to 1024/8 = 128\n",
    "        # The output feature map size will be: 128 channels, 128x128 spatial dimensions.\n",
    "        # Flattened feature size = 128 * 128 * 128\n",
    "        self.fc1 = nn.Linear(128 * 128 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 3, 1024, 1024)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)  # -> shape: (batch_size, 32, 512, 512)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # -> shape: (batch_size, 64, 256, 256)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # -> shape: (batch_size, 128, 128, 128)\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # -> shape: (batch_size, 128*128*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the network\n",
    "model = SimpleCNN(num_classes=10).to('cuda:0', dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDED 0\n",
      "ENDED 1\n",
      "ENDED 2\n",
      "ENDED 3\n",
      "ENDED 4\n",
      "ENDED 5\n",
      "ENDED 6\n",
      "ENDED 7\n",
      "ENDED 8\n",
      "ENDED 9\n",
      "ENDED 10\n",
      "ENDED 11\n",
      "ENDED 12\n",
      "ENDED 13\n",
      "ENDED 14\n",
      "ENDED 15\n",
      "ENDED 16\n",
      "ENDED 17\n",
      "ENDED 18\n",
      "ENDED 19\n",
      "ENDED 20\n",
      "ENDED 21\n",
      "ENDED 22\n",
      "ENDED 23\n",
      "ENDED 24\n",
      "ENDED 25\n",
      "ENDED 26\n",
      "ENDED 27\n",
      "ENDED 28\n",
      "ENDED 29\n",
      "ENDED 30\n",
      "ENDED 31\n",
      "ENDED 32\n",
      "ENDED 33\n",
      "ENDED 34\n",
      "ENDED 35\n",
      "ENDED 36\n",
      "ENDED 37\n",
      "ENDED 38\n",
      "ENDED 39\n",
      "ENDED 40\n",
      "ENDED 41\n",
      "ENDED 42\n",
      "ENDED 43\n",
      "ENDED 44\n",
      "ENDED 45\n",
      "ENDED 46\n",
      "ENDED 47\n",
      "ENDED 48\n",
      "ENDED 49\n",
      "ENDED 50\n",
      "ENDED 51\n",
      "ENDED 52\n",
      "ENDED 53\n",
      "ENDED 54\n",
      "ENDED 55\n",
      "ENDED 56\n",
      "ENDED 57\n",
      "ENDED 58\n",
      "ENDED 59\n",
      "ENDED 60\n",
      "ENDED 61\n",
      "ENDED 62\n",
      "ENDED 63\n",
      "ENDED 64\n",
      "ENDED 65\n",
      "ENDED 66\n",
      "ENDED 67\n",
      "ENDED 68\n",
      "ENDED 69\n",
      "ENDED 70\n",
      "ENDED 71\n",
      "ENDED 72\n",
      "ENDED 73\n",
      "ENDED 74\n",
      "ENDED 75\n",
      "ENDED 76\n",
      "ENDED 77\n",
      "ENDED 78\n",
      "ENDED 79\n",
      "ENDED 80\n",
      "ENDED 81\n",
      "ENDED 82\n",
      "ENDED 83\n",
      "ENDED 84\n",
      "ENDED 85\n",
      "ENDED 86\n",
      "ENDED 87\n",
      "ENDED 88\n",
      "ENDED 89\n",
      "ENDED 90\n",
      "ENDED 91\n",
      "ENDED 92\n",
      "ENDED 93\n",
      "ENDED 94\n",
      "ENDED 95\n",
      "ENDED 96\n",
      "ENDED 97\n",
      "ENDED 98\n",
      "ENDED 99\n",
      "ENDED 100\n",
      "ENDED 101\n",
      "ENDED 102\n",
      "ENDED 103\n",
      "ENDED 104\n",
      "ENDED 105\n",
      "ENDED 106\n",
      "ENDED 107\n",
      "ENDED 108\n",
      "ENDED 109\n",
      "ENDED 110\n",
      "ENDED 111\n",
      "ENDED 112\n",
      "ENDED 113\n",
      "ENDED 114\n",
      "ENDED 115\n",
      "ENDED 116\n",
      "ENDED 117\n",
      "ENDED 118\n",
      "ENDED 119\n",
      "ENDED 120\n",
      "ENDED 121\n",
      "ENDED 122\n",
      "ENDED 123\n",
      "ENDED 124\n",
      "ENDED 125\n",
      "ENDED 126\n",
      "ENDED 127\n",
      "ENDED 128\n",
      "ENDED 129\n",
      "ENDED 130\n",
      "ENDED 131\n",
      "ENDED 132\n",
      "ENDED 133\n",
      "ENDED 134\n",
      "ENDED 135\n",
      "ENDED 136\n",
      "ENDED 137\n",
      "ENDED 138\n",
      "ENDED 139\n",
      "ENDED 140\n",
      "ENDED 141\n",
      "ENDED 142\n",
      "ENDED 143\n",
      "ENDED 144\n",
      "ENDED 145\n",
      "ENDED 146\n",
      "ENDED 147\n",
      "ENDED 148\n",
      "ENDED 149\n",
      "ENDED 150\n",
      "ENDED 151\n",
      "ENDED 152\n",
      "ENDED 153\n",
      "ENDED 154\n",
      "ENDED 155\n",
      "ENDED 156\n",
      "ENDED 157\n",
      "ENDED 158\n",
      "ENDED 159\n",
      "ENDED 160\n",
      "ENDED 161\n",
      "ENDED 162\n",
      "ENDED 163\n",
      "ENDED 164\n",
      "ENDED 165\n",
      "ENDED 166\n",
      "ENDED 167\n",
      "ENDED 168\n",
      "ENDED 169\n",
      "ENDED 170\n",
      "ENDED 171\n",
      "ENDED 172\n",
      "ENDED 173\n",
      "ENDED 174\n",
      "ENDED 175\n",
      "ENDED 176\n",
      "ENDED 177\n",
      "ENDED 178\n",
      "ENDED 179\n",
      "ENDED 180\n",
      "ENDED 181\n",
      "ENDED 182\n",
      "ENDED 183\n",
      "ENDED 184\n",
      "ENDED 185\n",
      "ENDED 186\n",
      "ENDED 187\n",
      "ENDED 188\n",
      "ENDED 189\n",
      "ENDED 190\n",
      "ENDED 191\n",
      "ENDED 192\n",
      "ENDED 193\n",
      "ENDED 194\n",
      "ENDED 195\n",
      "ENDED 196\n",
      "ENDED 197\n",
      "ENDED 198\n",
      "ENDED 199\n",
      "ENDED 200\n",
      "ENDED 201\n",
      "ENDED 202\n",
      "ENDED 203\n",
      "ENDED 204\n",
      "ENDED 205\n",
      "ENDED 206\n",
      "ENDED 207\n",
      "ENDED 208\n",
      "ENDED 209\n",
      "ENDED 210\n",
      "ENDED 211\n",
      "ENDED 212\n",
      "ENDED 213\n",
      "ENDED 214\n",
      "ENDED 215\n",
      "ENDED 216\n",
      "ENDED 217\n",
      "ENDED 218\n",
      "ENDED 219\n",
      "ENDED 220\n",
      "ENDED 221\n",
      "ENDED 222\n",
      "ENDED 223\n",
      "ENDED 224\n",
      "ENDED 225\n",
      "ENDED 226\n",
      "ENDED 227\n",
      "ENDED 228\n",
      "ENDED 229\n",
      "ENDED 230\n",
      "ENDED 231\n",
      "ENDED 232\n",
      "ENDED 233\n",
      "ENDED 234\n",
      "ENDED 235\n",
      "ENDED 236\n",
      "ENDED 237\n",
      "ENDED 238\n",
      "ENDED 239\n",
      "ENDED 240\n",
      "ENDED 241\n",
      "ENDED 242\n",
      "ENDED 243\n",
      "ENDED 244\n",
      "ENDED 245\n",
      "ENDED 246\n",
      "ENDED 247\n",
      "ENDED 248\n",
      "ENDED 249\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dali_iter):\n",
    "    res = model(data[0]['images'])\n",
    "    print(f\"ENDED {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afilatov_fast_loading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
